---
title: SCALE - Causal Learning and Discovery of Robot Manipulation Skills using Simulation
author: Bruno Lee. J.
date: 2024-04-16 13:47:00 +0900
categories: [Paper, CRL]
tags: [causal inference, causal reinforcement learning]
pin: true
math: true
mermaid: true
---

## Abstract
저자는 제한된 데이터 세트에서 해석 가능한 다양한 로봇 스킬을 발견하고 학습하기 위한 접근 방식인 **SCALE**이라는 알고리즘을 제안한다.

여기서 데이터의 모든 모드를 포착하지 못할 수 있는 단일 스킬을 학습하는 대신에 먼저 인과적 추론인 **Causal Inference**를 통해 다양한 모드를 식별하고 각 모드에 대해 별도의 스킬 (skill)을 학습한다고 한다.

근데 여기서 얘기하는 **mode**가 무엇인지는 abstract에서는 무슨 의미인지는 파악하기가 힘들다.

<u>각 모드를 시뮬레이션에서 인과적 개입 (causal intervention)을 수행하여 발견한 인과적으로 관련된 고유한 컨텍스트 변수 세트와 연관시키는것</u>이 주요 인사이트라고 한다.

이를 통해서 데이터를 생성한 인과적 프로세스를 기반으로 데이터를 분할한 다음 관련 없는 변수를 무시하는 압축 스킬을 학습할 수 있다고 한다.

각 로봇 스킬을 지역별 압축 옵션으로 모델링하여 인과 프로세스와 관련 변수를 연결함으로써 옵션 프레임워크를 확장한다고 한다.

스킬 데이터 생성 영역으로 모델링된 각 인과 프로세스는 본질적으로 로컬이므로 컨텍스트 공간의 하위 집합에서만 유효하다고 한다.

불확실성 하에서 블록 스태킹과 페그인홀 삽입이라는 두 가지 대표적인 조작 작업에 대해 우리의 접근 방식을 시연합니다. 

실험 결과, 우리의 접근 방식은 컴팩트하고 도메인 이동에 강하며 시뮬레이션에서 실제로의 전송에 적합한 다양한 기술을 생성하는 것으로 나타났다고 한다.


## Introduction
최근에 Household 로봇이라는 모바일 매니퓰레이터 (mobile manipulator)를 이용한 연구를 많이 하고 있다. 이런 연구처럼 로봇이 집, 주방 식당에서 인간을 돕고 함께 일하기를 원한다. 하지만, 현재 로봇의 수준은 학습된 환경이 아니면 인간이 쉽게 할 수 있는 일조차 쉽게 하지 못하는 경우가 없다. 

그렇다면 인간이 훨씬 더 나은 이유가 무엇일까?? 
1. 인간은 일반적으로 실패와 환경의 변화에 견딜 수 있는 여러가지 해결법을 알고 있다. (e.g., 단단한 병뚜껑이 맨손으로 잘 안열리면 천 조각을 사용하여 그립감을 향상시킬 수 있다.)

2. 인간은 작업과 관련된 환경의 일부에만 선택적으로 주의를 기울이는데 탁월하다.

이러한 선택적 주의는 추론의 계산 복잡성 (computational cost)을 크게 줄여주고 복잡한 상황을 처리할 수 있게 해준다.


## Related Work

## Preliminaries
로봇의 구성요소는 skill set인 $K=\{K_1, ..., K_K\}$로 구성되며, 각 스킬은 작업들의 분포를 해결한다. 각 작업은 *manipulation MDP*로 모델링된다고 하며 $M := (S,A,R,T,\gamma, \tau)$에서 $s\in S$는 state space, $a \in A$는 action space, $R$은 reward function, $T$는 transition function, $\gamma$는 discount factor, $\tau$는 additional task information이라고 한다. 여기서는 최종 보상인 $R_f > R_s$이면 task가 해결되었다고 정의하였으며, $R_s$는 해결 threshold값이라고 한다.

여기서는 크게 5개의 범주로 나눠서 사전에 알아야 할 내용을 정리한다.

1. **Options**: 각 스킬 $K$는 parameterized된 옵션이다. Option $O := (\pi, I, \beta)$는 세가지 구성요소를 사용하여 정의된다.
    
    1) option 제어 정책 $\pi(a | s)$
    2) option 정책 $\pi$를 사용하여 state $s$에서 option $O$를 취할 때 최종 보상 $R_f$가 과제를 해결하는 위치를 시작 집합 $I = \{ R_f > R_S|s \}$
    3) Option이 종료되는 시점을 지정하는 종료조건인 $\beta(s)$

2. **Context**

3. **Contextual policies**

4. **Compressed Context and Feature Selection**

5. **Causal Reasoning in Simulation**



## Skill Formulation
1. **Regional Compressed Option**
본 논문에서는 skill $K$를 *Regional Compressed Option* (RCO)으로 공식화하는데, 여기서 $K:=(\pi_k \textnormal{Pre}, \beta, D)$이고 $\pi_k$는 option 제어 정책, $\textnormal{Pre}$는 *precondition*, $\beta$는 *termination condition*, $D$는 *data generating region* (DGR)이다. 


## Skill Discovery through Causal Reasoning in Simulation

## Experimental Results

## Conclusion